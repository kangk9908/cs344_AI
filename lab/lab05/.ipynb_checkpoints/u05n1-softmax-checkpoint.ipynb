{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax, part 1\n",
    "\n",
    "Task: practice using the `softmax` function.\n",
    "\n",
    "**Why**: The softmax is a building block that is used throughout machine learning, statistics, data modeling, and even statistical physics. This activity is designed to get comfortable with how it works at a high and low level.\n",
    "\n",
    "**Note**: Although \"softmax\" is the conventional name in machine learning, you may also see it called \"soft *arg* max\". The [Wikipedia article](https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=1065998663) has a good explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function defines `softmax` by using PyTorch built-in functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_torch(x):\n",
    "    '''Compute the softmax along the last axis, using PyTorch'''\n",
    "    # axis=-1 means the last axis\n",
    "    # This won't matter in this exercise, but it will matter when we get to batches of data.\n",
    "    return torch.softmax(x, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on an example tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tensor([1., 2., 3.])\n",
    "softmax_torch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by playing with the interactive widget below. Describe the outputs when:\n",
    "\n",
    "    1. All of the inputs are the same value. (Does it matter what the value is?)\n",
    "        - **It does not matter what the value is. The bar graphs show the same output.**\n",
    "    2. One input is much bigger than the others.\n",
    "        - **The remaining other bar graphs become shorter.**\n",
    "    3. One input is much smaller than the others.\n",
    "        - **The remaining other bar graphs become larger.**\n",
    "\n",
    "Finally, describe the input that gives the largest possible value for output 1.\n",
    "- **Output 0 : -2.0**\n",
    "- **Output 1 : 2.0**\n",
    "- **Output 2 : -2.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2161e284bd4d2b8fdf215bffbc7281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='x0', max=2.0, min=-2.0), FloatSlider(value=0.0, descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 2.0 # specify the range of the sliders\n",
    "@widgets.interact(x0=(-r, r), x1=(-r, r), x2=(-r, r))\n",
    "def show_softmax(x0, x1, x2):\n",
    "    x = tensor([x0, x1, x2])\n",
    "    xs = softmax_torch(x)\n",
    "    plt.barh([2, 1, 0], xs)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.yticks([2, 1, 0], ['output 0', 'output 1', 'output 2'])\n",
    "    plt.ylabel(\"softmax(x)\")\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fill in the following function to implement softmax yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(xx):\n",
    "    # Exponentiate x so all numbers are positive.\n",
    "    expos = xx.exp()\n",
    "    assert expos.min() >= 0\n",
    "    # Normalize (divide by the sum).\n",
    "    return expos / expos.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate `softmax(x)` and verify (visually) that it is close to the `softmax_torch(x)` you evaluated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate each of the following expressions (to make sure you understand what the output is), then evaluate `softmax_torch(__)` for each of the following expressions. Observe how each output relates to `softmax_torch(x)`. (Is it the same? Is it different? Why?)\n",
    "\n",
    "- `x + 1`\n",
    "- `x - 100`\n",
    "- `x - x.max()`\n",
    "- `x * 0.5`\n",
    "- `x * 3.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x - 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x - x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1863, 0.3072, 0.5065])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0024, 0.0473, 0.9503])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x * 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., -1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = tensor([1., 0.,])\n",
    "y3 = y2 - 1\n",
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y4 = y2 * 2\n",
    "y4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Are `softmax(y2)` and `softmax(y3)` the same or different? How could you tell without having to evaluate them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding or subtracting a constant essentially cancels itself out when applying it exponentials, so  *y2* and *y3* are the same**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Are `softmax(y2)` and `softmax(y4)` the same or different? How could you tell without having to evaluate them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**They are different because multiplication or division changes the exponential.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extension: Numerical Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task for Numerical Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. *Numerical issues*. Assign `x2 = 50 * x`. Try `softmax(x2)` and observe that the result includes the dreaded `nan` -- \"not a number\". Something went wrong. **Evaluate the first mathematical operation in `softmax`** for this particularly problematic input. You should see another kind of abnormal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., nan, nan])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = 50 * x\n",
    "softmax(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1847e+21,        inf,        inf])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. *Fixing numerical issues*. Now try `softmax(x2 - 150.0)`. Observe that you now get valid numbers. Also observe how the constant we subtracted relates to the value of `x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(x2 - 150.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Copy your `softmax` implementation to a new function, `softmax_stable`, and change it so that it subtracts `xx.max()` from `xx` before exponentiating. (Don't use any in-place operations; just use `xx - xx.max()`) Verify that `softmax_stable(x2)` now works, and obtains the same result as `softmax_torch(x2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(xx):\n",
    "    # Exponentiate x so all numbers are positive.\n",
    "    expos = (xx - xx.max()).exp()\n",
    "    assert expos.min() >= 0\n",
    "    # Normalize (divide by the sum).\n",
    "    return expos / expos.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 1.9287e-22, 1.0000e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_torch(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Numerical Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain why `softmax(x2)` failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is because the number was too large that could also be represented as infinity which would then lead to a divide by zero error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use your observations in \\#1-2 above to explain why `softmax_stable` still gives the correct answer for `x` even though we changed the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is because we are taking the sum of everything which in the grand scheme of things, does not matter...?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain why `softmax_stable` doesn't give us infinity or Not A Number anymore for `x2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is becuase the expos is calculated after subtracting the maximum value of `xx` which would eliminate the `nan` issue**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension *optional*\n",
    "\n",
    "Try to prove your observation in Analysis \\#1 by symbolically simplifying the expression `softmax(logits + c)` and seeing if you can get `softmax(logits)`. Remember that `softmax(x) = exp(x) / exp(x).sum()` and `exp(a + b) = exp(a)exp(b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
